---
title: "Guidelines for code"
format: html
custom-callout:    
  good:
    icon-symbol: "âœ…"
    title: " Good"
    color: "#0b9444ff"
  bad:
    icon-symbol: "ðŸš«"
    title: " Avoid"
    color: "#d9325eff"
filters:
  - custom-callout
---

# 0. Motivation

We want our work at HIP to be accurate, transparent, and convincingâ€”including the code we write. This document describes some guidelines to achieve those goals, and points out some common pitfalls and effective ways to avoid them.

To set the stage, imagine the following scenario:

1.  You get brought on to an existing project
2.  You write some code to handle some task
3.  You get pulled to another project
4.  Ten months later, you're asked to revise the code to do something slightly different

This is a common scenario that can give rise to difficulties, for example:

-   Figuring out where the code lives
-   Figuring out what some particular script does in relation to the others
-   Figuring out what the data sources are and if they need to be changed
-   Figuring out what object and variable names mean
-   Figuring out why this code was written in a particularly way

This may be the case whether you're reading code from past-you, or from someone else. But the good news is that most of these pain points can be prevented or mitigated by following some good practices from the beginning.

The intent of this guide is to present a set of good, generally applicable practices to help you write clear, replicable, and accurate code. The examples come from R, but the general principles hold regardless of programming language.

# 1. Good code

When we perform research, we want the results to be reliable. This is of course true for any important task, but is especially important in our research: we want to present conclusions and recommendations that are based on transparent and thoughtful analysis of data. We do not want policy decisions to be influenced by error-laden or careless research.

*Good code* means code that leads to reliable and meaningful analyses. It should take the analysis step-by-step from the raw data source to the communication of the results, in a way that any careful reader can understand.

Think back to the hypothetical scenario from the top of the page, and think about what would make your life (and your colleagues' lives) easier or more difficult.

Here are some important heuristics to keep in mind throughout a project. If your code reflects these qualities, your life is likely to be much easier down the road.

-   Code should be **clear**: it should be easy to read your code, and your code should not be ambiguousâ€”either to the computer or to a human. 
-   Code should be **replicable**: it should work regardless of who is running it on what machine, and the results should be the same.
- Code should be **accurate**: it should work as intendedâ€”what it does should align with what it was meant to do.

As you write your code, know that others will want to look at your code and have the code make sense to them. Keeping clean code consistently can be an extra effort, but it will make for a better and more professional product, which will ultimately make the project an easier and more rewarding one.

# 2. Basics

We start with the basicsâ€”clear writing. The reason is this: before anyone (including a future version of you) can begin to understand any code, they must be able to read it. Code is almost inherently difficult to read because writing for computers is generally not how humans understand natural language. But there are some basic principles that can help you write more legible code.

Most of what follows is conventional: you could use a different set of conventions if you wanted. The most important principle is to be consistent. However, the following conventions are the official standards for writing R code for the tidyverse packages, and should be followed when writing in R. (If writing in other languages, you should pick a widely used style guide for that language and follow it.)

For more, see the [tidyverse style guide](https://style.tidyverse.org/).

## Usage and style

These are the most important conventions for legibility:

- Code blocks defining an object (like a pipeline) should have line breaks around them. But if there are multiple single-line statements that thematically relate, no line breaks among them are needed.

- There should be one space around every operator (like `%>%`, `+`, `-`, `/`, `=`, `%in%`).

::: good
`sum_amount <- 1 + 3`
:::

::: good
`ggplot(aes(x = category, y = percent))`
:::

::: bad
`mean(x, na.rm=TRUE)`
:::

- If a function call is longer than about 88 characters, it should be broken up into separate lines to fit the screen if possible. One line per argument is a good idea.

- Parentheses around function calls should have no spaces surrounding them.

::: good
`max(x)`
:::

::: bad
`max ( y )`
:::

- But parentheses for control statements (like `if`) and for bracketing statements (like `(1 + 1) / 10`) should have surrounding spaces.

- `TRUE` and `FALSE` are special keywords and should always be written out fully in all-caps in R, not as 'T' or 'True'.

- If you load a package with `library()` (which should always come at the beginning of a script), you should not specify the package in the code (e.g., `dplyr::select`). Only use the `::` notation if you are using a one-off function without loading the package, or if you specifically need it to avoid function name conflicts among packages you've loaded.

## Naming

You should use `snake_case` in naming variablesâ€”snake_case has good legibility and is less difficult to misspell than some other alternatives.

-   `camelCase` doesn't have helpful underscores between them to separate words, and is not as apparent if you misspell (for example by missing a cap).
-   `dot.case` is fine, but in some programming contexts dot-separation has a technical reserved meaning so can be potentially confusing.
-   `runoncase` is illegible.

You should *not* mix cases, either within a name (ðŸš« `Object_Name`) or within a script (ðŸš« `object_name_one`, `objectNameTwo`). Dealing with object names can be a pain for both readers and writers, so you should make naming style as simple as possible.

Over and above the visual conventions, a great name will be:

-   Meaningful
-   Unambiguous
-   Short

However, optimizing for all 3 qualities is difficult. The strong recommendation is to favor meaningfulness and unambiguity, even at the cost of length. A long variable name is annoying, but a confusing variable name is dangerous.

::: good
`tract_geoid_2022`
:::

::: bad
`trc_GEOID_22`

There are at least three things that are not ideal about the example above. What are they?
:::

Using ad-hoc abbreviations in variable names is doubly problematic because it's both difficult to understand (What does `trc` mean? Can you expect that everyone reading the code will know?) and easy to mistype (Which letters are left out? If you miss a letter, will you be able to tell easily?). If in doubt, spell it out.

If you have a series of variables that are thematically related, give them the same structure so the reader has a handle on what they are.

::: good
`tract_geoid_2022`, `tract_geoid_2023`, `tract_geoid_2024`
:::

::: good
`geoid_tract_2022`, `geoid_county_2022`, `geoid_state_2022`
:::

::: bad
`tract_geoid_2022`, `tract_2023_geoid`, `geoid_tract_2024`, all in the same dataset
:::

Also consider how names might be interpreted (or misinterpreted) down the road, especially by someone who does not know the underlying data well. People will make assumptions about things based on what they are called, so take care that your names do not lead people to make inaccurate assumptions about what your objects represent. If in doubt, err on the side of making names too precise, rather than being too interpretive.

The same conventions also apply for naming files (either for script names or for outputs). Avoid spaces and special characters other than `-` or `_` in filenames, as such names cannot be easily referenced in command line and for Git.

::: {.callout-caution}
Be aware that Windows *still* has a low character-length limit for filenames, so avoid deeply-nested directory structures or very long filenames.
:::

::: {.callout-warning}
Some file formats, like ESRI's shapefile format, have restrictive variable name length limits. [Do not write out to these formats](http://switchfromshapefile.org/).
:::

## Commenting

Commenting your code is a requirement to make it understandable.

Immediately when you start a new script, include a heading at the top named 'Purpose', with a description of the purpose of the script written out in a short paragraph. If the purpose changes as you go along, update this comment.

In the body of the script, section titles are a good way to orient the reader. Each new section (e.g., "Read in data", "Process data", "Output cleaned data") should have a corresponding comment line. Within each section, major steps (like starting a new pipeline) often should have a short 1-line comment.

::: {.callout-tip}
Use the cmd/ctrl-shift-R shortcut in RStudio to insert a commented section heading. In RMD or Quarto files, name the R chunks instead, as needed.
:::

Occasionally, some intermediate steps may benefit from their own comment lines. For example, you might do a particularly tricky conditional `mutate` and a comment might clear up some of your reader's questions preemptively.

An important case: if you're changing the number of observations in your data, or changing the number of NA values in a variable, you should insert a comment indicating how many rows were affected, because this information can be critical to any further processing steps either now or in the future. The `tidylog` package (which you should always use) will generate this log for you automatically, which you can copy-paste in as a comment.

::: {.callout-tip}
Use the cmd/ctrl-shift-C shortcut in RStudio to toggle a line (or block) of code in and out of comment status.
:::

Think of comments as descriptions of *why* you did what you did (and sometimes *what*), rather than *how*. The *how* should be apparent from reading the code itself (and if it isn't, think about how you could write the code to make it clearer).

## Miscellaneous

### Dates

When using dates and times in variable names or filenames, always use the [ISO 8601](https://xkcd.com/1179/) standard (YYYY-MM-DD for dates). This way, when you sort by name, the names will also be sorted chronologically. Only use dates in non-ISO 8601 formats in code when needing to write in 'regular' English, like in body text or plot labels (and in this case, write out the month names so that it's unambiguous internationally).

### Security

Do **not** expose any private information in your code. This includes any PII (personally identifiable information) and any API keys or passwords you may use for 3rd-party services.

If you need to refer to PII (e.g., to reference a specific address), use an anonymous key instead (e.g., a participant ID).

API keys should be stored in your .REnviron file and be called from there. Packages like `tidycensus` will automatically call your key from the .REnviron file. API keys are private information and you don't want others to potentially misuse your API key. The easiest way to edit your .REnviron file to run `usethis::edit_r_environ()`.

# 3. Project and script organization

We've discussed some general guidelines for clearer code, but how you organize your project files is also important. Below are some considerations.

## Version control

If the project you are working on has a Github repository, use itâ€”all your code for that project should live in that repo. Speak with your colleagues on the project beforehand to make sure how you plan to organize your files within the repository makes sense, and to ascertain the best way for you to manage your contributions (e.g., using branches named in a certain way).

If your project does not yet have a repository, it probably should. Speak with your colleagues about the structure that makes the most sense for your project.

When you're working on a task, it is best practice to use a branch that corresponds to that task. When you're finished with the task, use the pull request system to coordinate the review of your code and the incorporation of your changes into `main`.

You should use the [Github Desktop app](https://desktop.github.com/download/) to manage your interface with Git, rather than the built-in RStudio interface (which is missing several important features).

For more on Git and Github, see the [Using Github](using-github.qmd) page on this website.

## Ordering and organizing

How exactly your scripts should be organized will vary greatly by project. Check in with your colleagues from time to time about the organization of project files. Usually this is not considered a particularly exciting topic, but good organization that adapts to changing circumstances is well worth putting in the occasional time. Remember that projects often do not go away as scheduled! Someone (probably you) may need to revive the project 6 (or 10, or 24) months later. The less you have to reconstruct the project then, the better.

One generally applicable principle, however, is to keep major steps of the data science pipeline (cleaning, EDA, analysis, modeling, communication) separate. In a smaller project, this might be one script or RMD per step; in a larger project, each step may have multiple scripts organized in directories.

::: {.callout-tip}
If there are multiple scripts per directory, prefix them numerically (starting with '00\_' or '01\_') so that they display in order. The order should be the order the scripts should be executed in.
:::

Remember that the data that you input/output from your scripts into Box should also be well organized. Keep a separate readme in the project data folder describing each of the datasources as well as the intermediate outputs, as well as a link to the project repo. The readme in the project repo should also have a link to the Box data folder.

## Scripts vs reports

Generally, if what you want to do is communicate some aspect of our data (EDA, or a polished analysis) to a larger audience (the rest of your team or to the public), you want to write in a format that supports interleaving code and expository writing: for R, this will be Quarto or R Markdown.

However, for material that is not explicitly communicative in nature, I recommend using regular .R scripts instead. This is because RMD/Quarto files are paradoxically *less* legible and more cumbersome if you're interested only in the code (for example, RMDs will not have syntax highlighting when viewed on Github).

# 4. Important pitfalls and how to avoid them

In this section, we move to more conceptualâ€”but still fundamentalâ€”issues that are relevant to how you should approach any analysis. As you move through a project, keep your brain engaged to make sure you continually understand what's going on with your data and analysis. The following are some specific things to keep track of, but the list is not exhaustiveâ€”are there any items you think should be added to the list?

## Know your data!

The very first step is to know what you're working with: that should be your first goal when you start a project. Make sure you understand the following:

### What's the data for?

#### The goals of the project

What's the purpose of the research? What are the research questions? What facts will you need to answer these questions? Make sure you are on the same page as as your colleagues.

#### Domain-specific information

What program, policy, or entity is this data about? Get background knowledge. If the data come from a program, what does the program do? Who's eligible? Who funds it? When did it start? Ask your colleagues to share with you any background material.

#### The data source

Who gave you the data? Who collected it? How did you get it? Were the data compiled by someone before you got it? If so, who did it and how was it done? Often, the datasource will be a project partner, or a colleague. Again, ask your colleagues; see if any questions about the data can be addressed by asking the datasource.

#### Metadata

This can be a data dictionary, a description of the data collection procedures, survey questionnaires, or even notes from meetings with the datasource. But be careful! Just because there *is* a data dictionary, it doesn't mean that it's *accurate*. Data dictionaries are seductive because they promise authoritative information, but they are fallible. The definitions of fields often change as data are collected. There can also be outright errors in data dictionaries, even from venerable sources like the Census Bureau. Verify before you trust.

### What's in the data?

Expanding upon this theme: it is unwise to assume anything about the data that you haven't checked out for yourself. This is especially the case if you think you *do* have reasons to trust the datasource. Whenever you work with a dataset, you should convince yourself that you understand the following:

#### What you have

Do you have the number of rows you expect to have? Is the file suspiciously too large or too small? Is it older or newer than you expected? Is the filename weird? Are there any warnings when you import the data?

If your dataset is less than \~1000 rows and \~50 columns, it's a good idea to manually skim through the entire dataset when you first import it. If your dataset is larger, look first at a few hundred contiguous records, and then pull out a random chunk (e.g., with `dplyr::slice_sample`) to manually examine.

If your data are updated with newer data, you should be ready to encounter new anomalies; have procedures in hand to check incoming data before you rely on them.

#### What represents a record

What do you expect each row to represent, and is this actually the case? If there is a unique ID, is it *actually* unique? If there isn't one, can you reliably create one from the data, or do you need more information? Are there duplicates among the rows which should belong to a unique record? If there are, why? And what do you need to know to be able to understand the duplicates and resolve them? (A helpful tool for this is `janitor::get_dupes()`.)

#### What are the record attributes

In short, what are the columns? Does the formatting of the column names make sense? (If they do, you can use `janitor::clean_names()` to standardize them to snake_case. If they don't make sense, you should investigate further.) How many columns are there and what are the types? (You can use `skimr::skim()` to answer this question.) If the data types are not what you expect (e.g., ZIP codes stored as numerics or dates stored in Excel format), are they fixable? Are there any columns which appear duplicated, or could refer to the same thing? If so, verify that they are actually exactly the same, or find out exactly how they differ.

::: {.callout-warning}
Be careful about assuming data types based on what they look like. A GEOID column, for example, may only have numeric digits but should be stored as strings. Check the types explicitly.
:::

#### Missing data

How does your dataset encode missing data? In some datasets, missing data are specific values, like '-b' or '-1'. Sometimes missing values can vary by variable, like the lowest possible value for that column minus one. In others, it may be keywords like NULL, Null, NaN, nan, N/A, n/a, N/a, or any variation thereof. Often, a dataset will have several different ways to encode NAs.

One of the first things you should do is to standardize all missing values (but *only* the missing values) to R's `NA` keyword. Once that's done, are there any rows or columns that are completely NA? (`janitor::remove_empty()`, with verbosity on, is helpful.) If so, is it safe to remove them? Inspect the `skim` output and notice any variables with large NA percentages. 

But just as suspicious are variables with zero NAs: were missing values for these variables coded badly (e.g., did the datasource assume that NA means FALSE?)? In some cases, it might be ambiguous whether a value like `0` is really a zero or actually represents missing or inapplicable data. In such a case you may need to ask the datasource.

#### Unexpected values

For numeric variables, sort by ascending and descending and notice any outliers. A simple histogram can also be a good tool. Are people reporting negative number of children? Is there anyone earning \$10 million in income? Are 30% of the rows reporting \$0 in rent? Are there implausible dates? For categorical variables, how many unique values are there for each variable? How common is each unique variable value? The `skim` output, and `janitor::tabyl()`, can get you quick initial answers for many of these questions. You may also find some more issues with nonstandard NA values at this point.

#### Geographic data

For any datasets with geospatial information, make sure you know which coordinate reference system ([CRS](https://r-spatial.org/book/02-Spaces.html#sec-crs)) is applicableâ€”don't assume, for example, that a GEOJSON file has coordinates in WGS84 (even though it should). Before you work with geospatial data, map the data and check they're located where you expect them to be (e.g., Philadelphia, not [Null Island](https://en.wikipedia.org/wiki/Null_Island)).

#### Relationship with other data

If records in the current dataset need to be joined to records in another, is there a common join key? If there is, how many fail to join from each dataset and is there an explanation why? If there is not a pre-existing join key, do you have the information you need to construct one? If a field in your current dataset also exists in another dataset, do they agree with each other, for every matched record?

### The data cycle

There's a common saying that 90% of data work is data cleaning and 10% is analysis. That's false. It's closer to the truth to say that 50% is understanding what the data contain in the first place, 45% is cleaning, and 5% is analysis, though the exact proportion will differ between projects.

There is an order of operations: You should start getting to know the data before you clean it, and you should be comfortable with data quality before you do any analysis. But the work will be iterativeâ€”you may find some unexpected pattern while you clean, and structuring an analysis may suggest ways to understand the data differently. Don't be afraid of revisiting prior steps with better knowledge, but at the same time, understand that decisions you make at the earliest stages are also the most difficult to change later. Keep your colleagues updated throughout so that everyone on the project is on the same page.

## NA handling

On to a new topic. How missing data are treated in data processing steps is an important but easily-neglected issue: mistakes in this arena can introduce consequential errors, but are very easy to miss unless you're looking for them. 

Once you understand the missingness patterns in your data (see [above](code-guidelines.qmd#missing-data)), make sure that your code handles them properly. How R handles missing data can be unintuitive at times, so whenever you construct any logical comparison, or generate any summary, make sure you you're satisfied how your code dealt with NAs.

Remember, NAs are not valuesâ€”they are labels for `we don't know`. That's why `NA == NA` is a meaningless expression and we use `is.na()` instead.

The following are common examples of how NA errors can creep in when you don't watch out for them:

-   Be very careful when `filter`ing based on columns that contain NAs: rows will always drop if the logical comparison evaluates to NA.
    -   For example, if you want to drop anyone younger than 18, `filter(data, age >= 18)` will also drop any records with NA for `age`. Almost never do you actually want this, so you'll need to write instead `filter(data, age >= 18 | is.na(age))`.
    -   If you *do* want to drop rows with NAs, explicitly note this in a comment, and include in the comment how many rows are dropped.
-   *Most* R operations will propagate NAs (e.g. the `mean()` of any vector containing NA will itself be NA), but not always.
    -   The `%in%` operator will never return NA. Evaluating `NA %in% NA` will return `TRUE`! This may be what you want, but keep in mind that `%in%` is not interchangeable with element-wise `|`, which can return NA.
    -   Dplyr's `join` family will unfortunately match NAs in the join key(s) with each other, so that everyone with a missing join key in dataframe A will match with everyone missing a join key in dataframe B. This is the opposite of SQL behavior. If there's a chance that your join key(s) will include any NAs, set the argument `na_matches = "never"`.
-   Generally, don't forget that NAs can be introduced even if you don't specify such conditions explicitly. Don't expect `ifelse()` or `case_when()` to always output non-missing values.

To help mitigate any NA-driven errors, keep an eye on the `tidylog` output to make sure that your `filter` or `mutate` steps did what you expected. Whenever you introduce, drop, or transform NAs, explicitly note in a comment how many rows are affected.

## Dirty environments

Let's now look more closely at reproducibility. We write code so that our work can be performed again and again with the same results, across users and machines. Sometimes, this requires some pre-planning to achieve. Below are some requirements to keep in mind, divided by considerations that apply to code within a file, versus those that apply to the interrelationships between files.

### Within-file

#### Filepaths

Nearly every script you write will require the specification of some filepaths, in order to read in (or write out) data. Of course, the exact content of the filepath will differ between computers and between operating systems. This means that if you ever call a 'hard' (or 'absolute') filepath in your script (e.g., `/Users/yourname/Documents/Projects/Repos/code-guidelines`), it will by definition not work for anyone not using your computer.

There are two methods that can deal with this. If your project and all of the needed data can live in one directory (or be called by API), the easiest solution is to use an R project. When you create an R project, and open associated scripts in the R project window in RStudio, the R project will bring along its own location so that only relative paths need to be called, which will be the same for everyone using the project directory. [This](https://martinctc.github.io/blog/rstudio-projects-and-working-directories-a-beginner%27s-guide/) is a useful short guide to using R Projects. However, this situation will rarely be the case at HIP.

Usually, your data will need to be read from Box, not from within your project directory. For our work at HIP, any data containing sensitive information must be stored on Box; therefore, most projects tracked under Git will need to interface with Box. An API could be used to do this, but the simpler solution is to use Box Drive to enable users to call locations on Box using a regular filepath, and to store the locally-specific part of the Box Drive filepath as a local environment variable in the .REnviron file. The [Getting Set Up](getting-set-up.qmd#box) page on this website tells you how to set this up.

::: {.callout-warning}
When storing or writing any path, use the unix directory separator `/` instead of the Windows separator `\`. The former can be read cross-platform in R but not the latter.
:::

#### Writing in place

It is unfortunately possible create an object, and later alter the object, under the same name. For example:

```{r}
#| eval: false

data <- data.frame(a = c(1, 2, 3, 4))

data$b <- c(10, 11, NA, NA)

data <- dplyr::filter(data, !is.na(b))

```

In this example, the same object named `data` is created or modified at three different places. The example above may look relatively harmless, but is in fact very risky. Consider that dozensâ€”or hundredsâ€”of lines of code may intervene between the three statements. Would you be confident of knowing at any given point whether `data` has 1 or 2 columns, or whether it has 4 rows or 2? 

Consider that writing code almost always develops interactively, and you may go back and forth between different sections of your script, continuously accumulating unreproducible changes to `data`. You will lose track of what `data` contains under such circumstances. This will then lead to code that doesn't workâ€”or worse: works but without you realizing it does something completely different than what you intended.

Modifying an existing object without assigning it a new name is called 'writing in place'. This is dangerous. **Simple rule: *never* write in place.**

::: {.callout-important}
Never make changes to an object without assigning it a different name.
:::

The names of your objects should always have a 1-to-1 relationship with their states. If the state of your objects is not clear, not only will it cause confusion for you and your reader, it is likely that errors will be introduced which are difficult to fixâ€”and worse, difficult to spot.

Instead, always assign new names to objects when you change them. The example above could be fixed like this:

```{r}
#| eval: false

data_raw <- data.frame(a = c(1, 2, 3, 4))

data_prepared <- data_raw %>% 
  mutate(b = c(10, 11, NA, NA)) %>% 
  # filter: removed 2 rows (50%), 2 rows remaining
  filter(!is.na(b))

```

This way, an object will have only one set of contents, always.

In R, pipes let you do this very naturally and simply. Sometimes the rule of avoiding changing in place means that pipelines may need to be broken out if you need some intermediate computation. That's fine: having to create one more name is small price to pay to achieve transparent code. When a name is only used once (remember, a pipeline is a single expression), you and your reader will know exactly what the name refers to at all times. It does mean that you may spend some more time thinking of good object names, but that is itself helpful in guiding the reader through what your code does.

::: {.callout-caution}
In other programming languages like Python, be aware that associating an existing object with a new name actually maintains the link to the old object, and changing the object under the new name also changes the object under the old name. This is one disadvantage of that language compared to R.
:::

#### RNG seeds

Some functions will use randomness to do their jobs. Of course, true randomness would be incompatible with reproducibility, but any function that uses a random number generator (really, a *pseudo*random number generator) can be constrained by setting the RNG seed beforehand, either by explicitly using `set.seed()` just before the function is called, or sometimes by setting a seed as a function argument. This will constrain the 'random' result of the function so that the result is the same across runs.

::: {.callout-warning}
Remember that a seed value set through `set.seed()` is 'consumed' when it is used, so it will need to be reset every time when an RNG is used, not just once at the beginning of the script.
:::

### Between-files

Any but the smallest projects will have more than one script associated with it. It's crucial for replicability that the relationship between the files is transparent and well-defined. Here are some principles to keep in mind:

1.  Scripts should be executed one at a time, and the order of execution should be reflected by the name of the script. As stated above, use numeric prefixes to indicate this. For more complex projects it's a good idea to have separate directories to separate scripts belonging to different stages in the data analytics workflow.
2.  Scripts should always have *explicit* inputs and outputs. A script should always start running from a fresh R session, with no objects loaded. Any output from a previous step should be written out in the earlier script as an intermediate and be read in from the later script.
3.  The state of your RStudio Environment panel should *never* be treated as authoritative. What's authoritative is your code, and any files that are written out from your code. What's real is what's on the hard drive, not what's in RAM.

::: {.callout-tip}
The shortcut cmd-shift-0 (ctrl-shift-F10) will restart R and clear your environment, right where you are without restarting RStudio. You should do this every once in a while while you're developing scripts to guard against unintended environment-related errors, which can be very difficult to ferret out. Always restart your R session before running a script to write out data.
:::

An important corollary is that the origin of any data files and intermediate files should be clear. A readme file specifying where each data/intermediate file comes from will accomplish this. It's also a good idea to organize intermediates with appropriate directory structures, corresponding to how your source scripts are organized.

::: {.callout-warning}
If an intermediate file contains PII (for example, cleaned addresses), it is sensitive data and can only be stored in Box.
:::

### Packages and functions

A full treatment of package environments in R is beyond the scope of this guide, but a few things to keep in mind are:

-   Avoid using functions or features that the package maintainer has labeled deprecated.
- Be careful of functions or packages that are under very active development, as future changes may break backwards compatibility with your code.
-   Tools like [`renv`](https://rstudio.github.io/renv/articles/renv.html) enhance reproducibility by letting you specify exactly which versions and sources your pakages come from.

## Grouped data

One last topic: grouped data. Remember that once a dataframe is grouped, the grouping is persistent, even if the object is copied to a different name. It is best practice to always `ungroup()` any dataframe immediately after you are done with operations that use the grouping. No dataframe should leave a pipeline in a grouped state. Very difficult-to-unravel bugs can be caused when you forget that a grouped dataframe is grouped and you apply a function that acts on a grouping that no longer makes sense.

::: {.callout-tip}
In many situations, using the `by =` or `.by =` argument to specify grouping within a particular function is more concise than calling `group_by()` then `ungroup()` one step later in the pipeline.
:::

# 5. Summing up

This is a long page, but it distills some of the lessons learned from many projects involving administrative, survey, and public data at HIP. 

One final tipâ€”please use your colleagues as a resource! Don't hesitate to discuss with your colleagues any roadblocks or anomalies that you come across; there might be a ready solution, or you might have uncovered an important unresolved issue. You never know until you ask.

In the meantime, check out the other pages on this website for other helpful tips about [RStudio](getting-set-up.qmd#tips), [Github](using-github.qmd#tips), or [useful R packages](resources.qmd).
