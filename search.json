[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Below is a list of resources that may be useful for your work.\n\nBooks\n\nR for Data Science\n\nThe best single-source reference for using R/Tidyverse in data analytics. If you haven’t read through it before, I highly recommend it.\n\nAnalyzing US Census Data: Methods, Maps, and Models in R\n\nAll you need to know about working with Census Bureau data (including ACS and PUMS) using Tidycensus and friends.\n\nSpatial Data Science With Applications in R\n\nThe main reference work for geospatial analysis in R, using the sf package and friends.\n\nTidy Modeling with R\n\nExplicates the Tidymodels package family, indespensible for modeling workflows in R.\n\n\n\n\nR packages\n\nTidylog\n\nA must-have. Tidylog prints out a log of most tidyverse operations in the console whenever you run them. Did this mutate operation introduce NAs? How many rows were dropped when I filtered? Does this left_join introduce duplicates? This is information that you need to know, and tidylog will helpfully serve it to you, saving you very tedious repeated work. Install the development version linked at the package git repo.\n\nJanitor\n\nProvides a wealth of very useful functions, including automatic variable name cleaning, deduplication management, and creating easy summary tables.\n\nSkimr\n\nA very convenient initial exploratory data analysis tool, providing a helpful overview of any dataframe.\n\nfastLink\n\nProvides a powerful probabilistic linkage algorithm for joining records.\n\nunpivotr\n\nPowerful package for tidy-ing grotesque Excel spreadsheets\n\n\n\n\nOther resources\n\nCensus Reporter\n\nA great readable reference for Census data products. The linked page lists ACS tables organized by topic. Want to know everything the ACS has on housing? Just check the Housing topic page.\n\nTIGERweb\n\nAn interactive interface for viewing Census geographic boundaries. A good tool to quickly check questions like: do PUMA boundaries overlap County X’s boundaries? How many census tracts are in City Y?\n\nProgramming with dplyr\n\nA reference page for handling tidyverse-special programming constructs like tidy evaluation in your functions. You will want to refer to this page when you write functions to encapsulate tidyverse pipelines (which will be often).\n\nTidyverse style guide\n\nStandards for neat code\n\nHow to Write a Git Commit Message\n\nTips for writing great commit messages\n\nCreate Quarto websites\n\nDetailed guide about how to creating Quarto websites, like this one"
  },
  {
    "objectID": "getting-set-up.html",
    "href": "getting-set-up.html",
    "title": "Getting set up",
    "section": "",
    "text": "The quantitative workflow at HIP depends on three external systems: R, Github, and Box. Here are some step-by-step guides to help get you set up, whether you’ve just joined our team or setting up a new machine."
  },
  {
    "objectID": "getting-set-up.html#install",
    "href": "getting-set-up.html#install",
    "title": "Getting set up",
    "section": "Install",
    "text": "Install\nThis one’s easy. R and RStudio should already be installed on your machine. If it’s not, contact the IT help desk at ithelp@design.upenn.edu.\n\n\n\n\n\n\nNote\n\n\n\nRStudio is an integrated development environment (IDE) focused on R, which is basically a highly-featured text editor bundled with an R console, data viewer, and other tools. You can use other IDEs like Visual Studio Code if you have a prior preference."
  },
  {
    "objectID": "getting-set-up.html#tips",
    "href": "getting-set-up.html#tips",
    "title": "Getting set up",
    "section": "Tips",
    "text": "Tips\nSince RStudio is going to be a daily tool for you, here are some quality-of-life tips:\n\nYou don’t want to waste your life typing out %&gt;% every time. You can use the standard keyboard shortcut (cmd/ctrl + shift + M), or configure the shortcut to your liking (Tools &gt; Modify keyboard shortcuts).\nThe same goes for the assignment operator (&lt;-), option/alt + - (though I prefer changing it to cmd + -).\nIf you use the shortcut cmd + return (or ctrl + enter), RStudio will run the statement (e.g., a pipeline) where your cursor is located.\nThe cmd/ctrl + option/alt + b shortcut runs the script up to the point of the cursor (R scripts only).\nSee this page for a list of other very helpful shortcuts.\n\nConsider making your own default templates for new R scripts and RMD files, pre-populated with an empty Purpose section and library calls to your favorite packages.\nI also recommend experimenting with the aesthetics of how your code is displayed (Tools &gt; Global options &gt; Appearance)— setting up code highlighting to your satisfaction can make a difference for legibility. You can also configure theme files for custom code highlighting options."
  },
  {
    "objectID": "using-github.html",
    "href": "using-github.html",
    "title": "Using Github",
    "section": "",
    "text": "This page describes why we use Github to organize our projects at HIP, and guides you through the workflow so you can feel comfortable using version control tools in your work.\nBut first, if you haven’t yet set up your Github account and installed Github Desktop, follow the steps here before continuing."
  },
  {
    "objectID": "using-github.html#repos",
    "href": "using-github.html#repos",
    "title": "Using Github",
    "section": "Repos",
    "text": "Repos\nA repository (repo, for short) is the main unit of organization for version control. You can think of a repo as all the code needed for a particular project—it can be as small or large as needed, but each project should get its own repo. A repo will have its own page on Github, which will show you the directories and files that are part of the repo, a readme telling you what the repo is about, a commit history page showing you what the repo looked like in the past, and links to associated project management tools like the issues page.\n\n\n\n\n\n\nTry it yourself\n\n\n\nCheck out what a repo’s page looks like; for example, the repo for this website.\n\n\nIt’s important to know, though, that the repo can live in multiple copies, or clones. The repo which is on the Github website (called the remote repo) is the reference version for practical purposes, but the version you clone to your own computer (see below) is an independent copy. This is why it’s important to distinguish between the version of the repo that lives on Github versus the version that lives locally on your machine: changing one will not affect the other, unless you tell Github to make the update (more on this below as well)."
  },
  {
    "objectID": "using-github.html#commits",
    "href": "using-github.html#commits",
    "title": "Using Github",
    "section": "Commits",
    "text": "Commits\nA commit is a snapshot in time of a repo. It tells you exactly what the repo looked like when the commit was made. You could think of a repo as basically a collection of commits, leading from one to the other. This means it’s easy to revert from one state to another, almost like a backup system. Commits are not created automatically, though—you create them explicitly when there is a meaningful snapshot to preserve."
  },
  {
    "objectID": "using-github.html#branches",
    "href": "using-github.html#branches",
    "title": "Using Github",
    "section": "Branches",
    "text": "Branches\nThe simplest kind of repo is like the trunk of a tree, with one commit linked to the next one in an unbranching chain. But Git is designed to work well with a branched structure, where different lines of commits can branch out from the main trunk (which we call main). Think of branches as separate lines of development, which allows work on different parts of the repo to happen in parallel. When the purpose of the branch is completed, they can be folded in to main, bringing all of its changes with it. This is an important aspect of how we use Github, and we’ll work more with branches below."
  },
  {
    "objectID": "using-github.html#create-a-repository",
    "href": "using-github.html#create-a-repository",
    "title": "Using Github",
    "section": "Create a repository",
    "text": "Create a repository\nThe very first order of business when setting up a new project repo is to discuss with your colleagues how your project should be structured. This might change as the project progresses, but everyone working on the same project should be on the same page before project infrastructure is created.\nOnce that is done, a repository should be created if it hasn’t been already. Head to our Github org’s repositories page and click the green ‘New Repository’ button. Note that the ‘Owner’ field is already set to the housinginitiative organization.\n\n\n\n\n\n\nNote\n\n\n\nAny repo created for HIP work should live in the housinginitiative organization, rather than belonging to an individual’s account.\n\n\nNext to that field is the ‘Repository name’ field. The convention here is to use all lowercase with hyphens as separators. When you start a project, a name should be chosen that will be used on all platforms referring to that project, including on Github; this should be one of the things that the project team should have decided on already.\nA repo can be public or private—private repos are only visible to members of the housinginitiative organization plus any external collaborators you add to that project; for everyone else, the repo page will return a 404 error. A project repo should be private (though your team may make it public later).\nYou should always enable the ‘Add a README file’ option, and add a .gitignore file for R. The README file creates a mini ‘front page’ for your repo which should describe what the repo is about. The .gitignore file is there so files that are specific to your system and not relevant for others’ machines are not going to clutter up the repo.\n\n\n\n\n\n\nTip\n\n\n\nYou can automatically apply this basic setup by simply using the template repo for our organization: at the top of the ‘Create a new repository’ page, select the drop-down menu under ‘Repository template’ and select housinginitiative/hip-template-repo, and just fill in your repo’s name. I recommend you use the template.\n\n\nOnce you create a repository, invite your team by clicking on ‘Settings’ near the top of the repo page and selecting ‘Collaborators and teams’ on the left-hand menu of the settings page. Currently, everyone part of the housinginitiative organization will be able to read the repo, but not write to it. Click ‘Add people’ and select from the menu. At least one other person from HIP should have the ‘Admin’ role; otherwise ‘Maintain’ or ‘Write’ is fine.\n\n\n\n\n\n\nCaution\n\n\n\nIf you add people by their username, make sure that it’s the correct username! Copy-paste directly from their user profile page.\n\n\nThe next order of business is to edit the project’s main README to include information about the project and include a link to the project’s Box directory. If you used the project template, there should placeholders already.\n\n\n\n\n\n\nTip\n\n\n\nIf you used the template repo, the .gitignore file includes an entry for .DS_Store, a ubiquitous user configuration file in Mac OS. If you did not use the template repo, you should edit the .gitignore file to include it—otherwise it will cause issues.\nMac users reading this guide are also recommended to edit their global .gitignore file on their machine so this file is ignored regardless of whether the repo’s .gitignore specifies it.\n\n\n\n\n\n\n\n\nTry it yourself\n\n\n\nNow try creating your own test repo for this guide! But create this repo under your own account, not the HIP organization."
  },
  {
    "objectID": "using-github.html#clone-the-repository",
    "href": "using-github.html#clone-the-repository",
    "title": "Using Github",
    "section": "Clone the repository",
    "text": "Clone the repository\nNow the repository has been created, or you have been added to one that already exists. But how would you access the files on your own computer? You could download files individually, but those files would be regular files and not tracked under version control. What you want to do is to create a linked copy, or a clone, of the repo on your local machine.\nThere are several ways to do this, but the simplest is just to click the green ‘Code’ button from the repo’s Github page, then select ‘Open with Github Desktop’.\n\nThat should open up Github Desktop—in the dialog window, select where on your computer the repo will be cloned to, then click ‘Clone’.\n\n\n\n\n\n\nTip\n\n\n\nI recommend creating a directory on your computer where you will keep all your cloned repos, for example Documents/Projects\n\n\nIn the Github Desktop window, you will see the current repo’s name in the upper-left corner. Left-clicking on that box will show you the list of all the repos on your computer. Right-clicking on that box will show you options to open the repo in your Finder/File Explorer, or to open the repo’s Github page. (These links are also in the main part of the Github Desktop window.)\nNote that the repo directory on your machine is a regular directory, like anything else on your computer—you can open files, add or delete files, edit files, or add subdirectories. However, this directory is also linked to Git, the version control system. Any changes you make to your directory will show up on the Github Desktop window for that repo, tracking all the changes you made. That also means that Git can change that directory when you perform Git actions like pulling or changing branches (see below).\n\n\n\n\n\n\nTry it yourself\n\n\n\nClone your practice repo that you created above, and open the repo directory in your file browser."
  },
  {
    "objectID": "using-github.html#create-a-branch",
    "href": "using-github.html#create-a-branch",
    "title": "Using Github",
    "section": "Create a branch",
    "text": "Create a branch\nAs we saw above, branches are a way to keep lines of development well organized. If you have multiple people working on a repo, it’s important to organize that work to prevent people from stepping on each other’s toes.\nBranches should be viewed as more-or-less self-contained mini-projects. A branch should generally address one issue (or a few small related issues), and have only one person working on the branch at a time. This way, each person working on the project has their own ‘box’ and it is easier to keep track of who is doing what.\nBranches can be created directly, by clicking on the ‘Current Branch’ tab at the top of the Github Desktop window, then selecting the ‘New Branch’ button. Give the branch a short but descriptive name for the task you want to get done on this branch, again using nothing but lowercase letters and hyphens. Once you create the branch, you should see a new option pop up on the third box at the top of the Github Desktop window: ‘Publish branch’. Click on this option, and Github Desktop will publish the branch to the remote repo on Github. (Remember, remote is the version of the repo that lives on Github, i.e., the cloud. Changes you make to your local repo won’t be propagated to remote unless you explicitly tell Git to do so.)\nWe can also approach this from the other way around. Instead of creating a branch and naming it after a task, we can first define the task then create a branch to address it. Github offers an excellent interface to do this via the Issues page, which you can access via your repo’s Github page, among the options on the top. On that page, you can add issues, describing what the issue is about and what needs to be done to complete it. Once you create the issue, you will see on the right side of the issue’s page an option to create a branch directly from the issue page. This is a great way to define the purpose of your branch before you create it.\n\n\n\n\n\n\n\nCaution\n\n\n\nAlways be aware of which branch you’re in. Moving between branches will change the files in your repo directory. Always save your work before changing branches!\n\n\n\n\n\n\n\n\nTry it yourself\n\n\n\nGo to your project’s Github page, create an issue, and create a branch from the issue."
  },
  {
    "objectID": "using-github.html#pull-in-changes-from-remote",
    "href": "using-github.html#pull-in-changes-from-remote",
    "title": "Using Github",
    "section": "Pull in changes from remote",
    "text": "Pull in changes from remote\nBefore you start on a day’s work, it’s a good idea to make sure your local repo is in sync with the remote repo on Github. If you are the only person working on a branch (which should usually be the case), there should not be changes to your branch on remote, but if you are accessing the repo from another computer or changes have accumulated on main, you will need to update the state of your local repo.\nYou can do this easily from Github Desktop. First, make sure you are in the branch you want. Then, click the ‘Fetch origin’ button at the top of the Github Desktop window to see if the remote repo has changed (yes, you need to ask directly whether changes have been made to remote; they will not be automatically fetched). If the branch has changed on remote, Github Desktop will show you the ‘Pull origin’ button: selecting this button will update your local repo directory with the changes. (For our purposes, origin is just another name for remote.)\nKeep in mind that ‘Pull origin’ only updates the branch you are in currently. Changes between branches will not be synced to each other unless you merge in the changes. Usually, branches will be short-lived and changes will not accumulate too much on other branches. But if there have been changes in main while you’re working on your branch, be sure to merge in main to your own branch (instructions here).\n\n\n\n\n\n\nImportant\n\n\n\nBut never merge in your branch directly to main! If you want to update main, you should always use pull requests to do so (see below).\n\n\n\n\n\n\n\n\nTry it yourself\n\n\n\nFetch and pull from origin. This should make the branch you just made on Github available on your local machine. Switch to that branch."
  },
  {
    "objectID": "using-github.html#make-changes",
    "href": "using-github.html#make-changes",
    "title": "Using Github",
    "section": "Make changes",
    "text": "Make changes\nThis is where you will spend the most of your time: developing your code.\nAs you do so, keep these things in mind:\n\nKeep your environment clean (see here for more). When someone else on your team pulls in your changes, they should be able to run your code out of the box.\nGithub is NOT appropriate for storing any private data: such data should only be stored on Box. Don’t include any private information in your code either (e.g., someone’s name or address).\nVersion control is a tool for keeping track of code. Small images or HTML outputs are fine to include, but don’t store large binary files (like Excel files) in your repo.\n\n\n\n\n\n\n\nTry it yourself\n\n\n\nNow that the preparation is done, let’s make some changes! Create a new file, and edit the README. Remember, your repo is just a directory with some special powers—you can find and edit your files like normal."
  },
  {
    "objectID": "using-github.html#commit-and-push",
    "href": "using-github.html#commit-and-push",
    "title": "Using Github",
    "section": "Commit (and push)",
    "text": "Commit (and push)\nWe saw above that commits are like snapshots that make up the content of the repo. But changes that you save to your machine’s disk are not committed. They’re just changes. To make a commit out of your changes, you need to tell Git to do so explicitly.\nThis is easy using Github Desktop: on the left-hand side of the window, you’ll see a list of files which have been changed. Depending on how they were changed, you’ll see different icons for deleted, added, or edited files. (If you don’t see your changes, make sure your changes are saved to disk.)\n\nYou can click on the different files to see the highlighted diffs: the differences between the previous commit and your current changes.\nBefore proceeding, make sure you are in the branch you are working in. If not, click the branch tab at the top of the Github Desktop window and select the branch you want to move to. When Github Desktop asks you if you want to bring your changes along, say ‘Yes’.\nTo create a new commit for these changes, write a short message in the box that appears below the list of changed files. This is your commit message.\n\nCommit messages are what you’ll read when you (or anyone else) browse through your repo, so it’s important that they are meaningful and readable. The convention here is to use present-tense verbs to describe your changes (for example, “Include Box link in README”). There is also a larger ‘Description’ box, where you can write a longer description if you want. This is optional, but the commit message is required. Once you’re happy with the commit message, click the blue ‘Commit to [branchname]’ button to create the commit.\n\n\n\n\n\n\nTip\n\n\n\nWithout commit messages, it would be nearly impossible to understand how a repo evolved. Because of this, commit messages appear everywhere on Github’s UI. Good commit messages are visible marks of professionalism: here are some helpful tips.\n\n\nThis is how you create a commit, but when you commit is also important. Commits should be meaningful in themselves—you don’t want to commit every 5 minutes and clutter up your repo’s history, but you also don’t want commits to be a smörgåsbord of different types of changes. The point above about commit messages is helpful here, too: you should commit when your changes add up to a good commit message. Generally, one to four commits a day might be reasonable.\n\n\n\n\n\n\nTip\n\n\n\nSometimes you might get carried away and make a number of changes that should belong to separate commits. In that case, you can select which changes you want to be part of the first commit, and commit the rest separately.\nIn Github Desktop’s ‘Changed Files’ window, check and uncheck which files you want to be part of the commit. (Unchecked changes are still saved and are available for future commits, unless you discard them.)\n\n\nOnce you’ve committed, it’s time to push your commit to Github—unless you do this explicitly, your changes will stay isolated on your machine. Github Desktop will helpfully surface this action for you at the top of the window.\n\n\n\n\n\n\nTry it yourself\n\n\n\nCreate one commit for the addition of the new file into your repo, and another for the edits to the README file. Then push."
  },
  {
    "objectID": "using-github.html#submit-a-pull-request",
    "href": "using-github.html#submit-a-pull-request",
    "title": "Using Github",
    "section": "Submit a pull request",
    "text": "Submit a pull request\nOnce you’re done with the task you created your current branch for, it’s time to incorporate your new commits to main.\nYou do this via pull requests (or PRs). PRs are a method to make your work available for inspection and comment before they are formally brought into main. Think of main as the ‘official record’, and PRs as draft reviews. A PR will include all the commits you’ve made so far in the branch, including all the diffs to changed files. A reviewer will be able to view the diffs and comment on them if desired. If you’ve ‘suggested edits’ or commented on a Google doc, this is the same principle, except that in this case the changes (your working branch) and the thing being changed (main) are kept separated until the PR is approved.\nWho is the reviewer? This will depend on the project. Generally, you should have at least one colleague who can and should review your code. Sometimes, you will be the only person working on a repo, in which case you should review your own code; though this is not ideal, the mechanism of the PR still enforces the concept of review and documentation, which is good.\n\n\n\n\n\n\nNote\n\n\n\nYou might be wondering why it’s called a pull request when you are requesting to push into main. This is because main is the permanent branch and so we take its perspective—from main’s perspective, the changes are being pulled into it.\n\n\nCreating a pull request is also easy from Github Desktop: when you have any commits in your branch, a prominent blue ‘Create Pull Request’ button appears on the Github Desktop window. Clicking this button opens a webpage where you can write a description and officially submit the pull request. The description here should concisely describe what your changes do, which gives your reviewer (or future you) the context needed to properly interpret your changes. This could be as simple as: ‘Completes issue #9’, or a longer statement if your task was complex or if you are uncertain whether your PR fully addresses the issue.\n\n\n\n\n\n\nTip\n\n\n\nIn the PR description field, using the hash sign # before a number will create a link to the issue page with that number.\n\n\n\n\n\n\n\n\nTry it yourself\n\n\n\nCreate a PR for your commits."
  },
  {
    "objectID": "using-github.html#review-and-resolve-the-pull-request",
    "href": "using-github.html#review-and-resolve-the-pull-request",
    "title": "Using Github",
    "section": "Review and resolve the pull request",
    "text": "Review and resolve the pull request\nOnce the PR is created, the reviewer (which could be you) should view the changes proposed in the PR and ask for any clarifications or changes, if necessary. To view PRs, click on the ‘Pull requests’ button near the top of the repo’s Github page. From there, view the PR description and click on the ‘Files changed’ tab to view the list of diffs. Additions will be highlighted in green, deletions in red. The list of files changed will be displayed to the pane on the left.\nIf you have a question or suggestion for the PR submitter, hover over the relevant line of code in the diffs to add a comment. Once you’re done with the comments, click the green ‘Start a review’ button.\n\nYou can also add general comments as a reply to the overall PR description.\nOnce you’re done with the review, let the submitter know so they can respond. The submitter can create new commits on the same branch the PR is from, and that will update the PR as well. The reviewer will see a ‘Refresh’ button if this happens and they haven’t refreshed the PR page yet. The cycle of comments and changes can continue until the PR is ready for approval. Once the reviewer approves the PR, the committed changes are merged into main.\n\n\n\n\n\n\nCaution\n\n\n\nOne potential complication: 95% of the time, if work has been well organized into branches, the merge should go smoothly. However, if the same file was changed in both of the branches being merged, a merge conflict may occur. In this case, the conflict must be manually resolved: the instructions are documented here. If there is a merge conflict, everyone should be on the same page about how it should be resolved.\n\n\n\n\n\n\n\n\nTry it yourself\n\n\n\nReview your own PR: look at the diffs, try out the comment function, then approve the PR."
  },
  {
    "objectID": "using-github.html#finish-up-and-continue-the-cycle",
    "href": "using-github.html#finish-up-and-continue-the-cycle",
    "title": "Using Github",
    "section": "Finish up and continue the cycle",
    "text": "Finish up and continue the cycle\nOnce the PR has been approved, the person who created the branch should delete it from both their local repo and from remote. This will keep the list of branches tidy. The associated issue should also be closed.\nAt this point, the project documentation (e.g., the README file) should be updated to reflect the new changes, if needed.\nAnd that’s it! Now the cycle is ready to begin again: discuss what the next tasks are, create issue pages for the tasks, start new branches, commit in new changes as needed, PR submission/review/approval, and clean up.\n\n\n\n\n\n\nTry it yourself\n\n\n\nDelete your branch and close the associated issue. If you have any questions, please reach out to Chi-Hyun."
  },
  {
    "objectID": "code-guidelines.html",
    "href": "code-guidelines.html",
    "title": "Guidelines for code",
    "section": "",
    "text": "We want our work at HIP to be accurate, transparent, and convincing—including the code we write. This document describes some guidelines to achieve those goals, and points out some common pitfalls and effective ways to avoid them.\nTo set the stage, imagine the following scenario:\n\nYou get brought on to an existing project\nYou write some code to handle some task\nYou get pulled to another project\nTen months later, you’re asked to revise the code to do something slightly different\n\nThis is a common scenario that can give rise to difficulties, for example:\n\nFiguring out where the code lives\nFiguring out what some particular script does in relation to the others\nFiguring out what the data sources are and if they need to be changed\nFiguring out what object and variable names mean\nFiguring out why this code was written in a particularly way\n\nThis may be the case whether you’re reading code from past-you, or from someone else. But the good news is that most of these pain points can be prevented or mitigated by following some good practices from the beginning.\nThe intent of this guide is to present a set of good, generally applicable practices to help you write clear, legible, and adaptable code. The examples come from R, but the general principles hold regardless of programming language."
  },
  {
    "objectID": "code-guidelines.html#usage-and-style",
    "href": "code-guidelines.html#usage-and-style",
    "title": "Guidelines for code",
    "section": "Usage and style",
    "text": "Usage and style\nThese are the most important conventions for legibility:\n\nCode blocks defining an object (like a pipeline) should have line breaks around them. But if there are multiple single-line statements that thematically relate, no line breaks among them are needed.\nThere should be one space around every operator (like %&gt;%, +, -, /, =, %in%).\n\n\n\n\n\n\n\nGood\n\n\n\nsum_amount &lt;- 1 + 3\n\n\n\n\n\n\n\n\nGood\n\n\n\nggplot(aes(x = category, y = percent))\n\n\n\n\n\n\n\n\nAvoid\n\n\n\nmean(x, na.rm=TRUE)\n\n\n\nIf a function call is longer than about 88 characters, it should be broken up into separate lines to fit the screen if possible. One line per argument is a good idea.\nParentheses around function calls should have no spaces surrounding them.\n\n\n\n\n\n\n\nGood\n\n\n\nmax(x)\n\n\n\n\n\n\n\n\nAvoid\n\n\n\nmax ( y )\n\n\nBut parentheses for control statements (like if) and for bracketing statements (like (1 + 1) / 10) should have surrounding spaces.\n\nTRUE and FALSE are special keywords and should always be written out fully in all-caps in R, not as ‘T’ or ‘True’.\nIf you load a package with library() (which should always come at the beginning of a script), you should not specify the package in the code (e.g., dplyr::select). Only use the :: notation if you are using a one-off function without loading the package, or if you specifically need it to avoid function name conflicts among packages you’ve loaded."
  },
  {
    "objectID": "code-guidelines.html#naming",
    "href": "code-guidelines.html#naming",
    "title": "Guidelines for code",
    "section": "Naming",
    "text": "Naming\nYou should use snake_case in naming variables—snake_case has good legibility and is less difficult to misspell than some other alternatives.\n\ncamelCase doesn’t have helpful underscores between them to separate words, and is not as apparent if you misspell (for example by missing a cap).\ndot.case is fine, but in some programming contexts dot-separation has a technical reserved meaning so can be potentially confusing.\nrunoncase is illegible.\n\nYou should not mix cases, either within a name (🚫 Object_Name) or within a script (🚫 object_name_one, objectNameTwo). Dealing with object names can be a pain for both readers and writers, so you should make naming style as simple as possible.\nOver and above the visual conventions, a great name will be:\n\nMeaningful\nUnambiguous\nShort\n\nHowever, optimizing for all 3 qualities is difficult. The strong recommendation is to favor meaningfulness and unambiguousness, even at the cost of length. A long variable name is annoying, but a confusing variable name is useless—or downright dangerous.\n\n\n\n\n\n\nGood\n\n\n\ntract_geoid_2022\n\n\n\n\n\n\n\n\nAvoid\n\n\n\ntrc_GEOID_22\nThere are at least three things that are not ideal about the example above. What are they?\n\n\nUsing ad-hoc abbreviations in variable names is doubly problematic because it’s both difficult to understand (What does trc mean? Can you expect that everyone reading the code will know?) and easy to mistype (Which letters are left out? If you miss a letter, will you be able to tell easily?). If in doubt, spell it out.\nIf you have a series of variables that are thematically related, give them the same structure so the reader has a handle on what they are.\n\n\n\n\n\n\nGood\n\n\n\ntract_geoid_2022, tract_geoid_2023, tract_geoid_2024\n\n\n\n\n\n\n\n\nGood\n\n\n\ngeoid_tract_2022, geoid_county_2022, geoid_state_2022\n\n\n\n\n\n\n\n\nAvoid\n\n\n\ntract_geoid_2022, tract_2023_geoid, geoid_tract_2024, all in the same dataset\n\n\nAlso consider how names might be interpreted (or misinterpreted) down the road, especially by someone who does not know the underlying data well. People will make assumptions about things based on what they are called, so take care that your names do not lead people to make inaccurate assumptions about what your objects represent. If in doubt, err on the side of making names too precise, rather than being too interpretive.\nThe same conventions also apply for naming files (either for script names or for outputs). Avoid spaces and special characters other than - or _ in filenames, as such names cannot be easily referenced in command line and for Git.\n\n\n\n\n\n\nCaution\n\n\n\nBe aware that Windows still has a low character-length limit for filenames, so avoid deeply-nested directory structures or very long filenames.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSome file formats, like ESRI’s shapefile format, have restrictive variable name length limits. Do not write out to these formats."
  },
  {
    "objectID": "code-guidelines.html#commenting",
    "href": "code-guidelines.html#commenting",
    "title": "Guidelines for code",
    "section": "Commenting",
    "text": "Commenting\nCommenting your code is an indispensable requirement to make it legible and understandable.\nImmediately when you start a new script, include a heading at the top named ‘Purpose’ or similar, with a description of the purpose of the script written out in a short paragraph. If the purpose changes as you go along, update this comment.\nIn the body of the script, section titles are a good way to orient the reader. Each new section (e.g., “Read in data”, “Process data”, “Output cleaned data”) should have a corresponding comment line. Within each section, major steps (like starting a new pipeline) often should have a short 1-line comment.\n\n\n\n\n\n\nTip\n\n\n\nUse the cmd/ctrl-shift-R shortcut in RStudio to insert a commented section heading. In RMD or Quarto files, name the R chunks instead, as needed.\n\n\nOccasionally, some intermediate steps may benefit from their own comment lines. For example, you might do a particularly tricky conditional mutate and a comment might clear up some of your reader’s questions preemptively.\nAn important case: if you’re changing the number of observations in your data, or changing the number of NA values in a variable, you should insert a comment indicating how many rows were affected, because this information can be critical to any further processing steps either now or in the future. The tidylog package (which you should always use) will generate this log for you automatically which you can copy-paste in as a comment.\n\n\n\n\n\n\nTip\n\n\n\nUse the cmd/ctrl-shift-C shortcut in RStudio to toggle a line (or block) of code in and out of comment status.\n\n\nThink of comments as descriptions of why you did what you did (and sometimes what), rather than how. The how should be apparent from reading the code itself (and if it isn’t, think about how you could write the code to make it clearer)."
  },
  {
    "objectID": "code-guidelines.html#miscellaneous",
    "href": "code-guidelines.html#miscellaneous",
    "title": "Guidelines for code",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nDates\nWhen using dates and times in variable names or filenames, always use the ISO 8601 standard (for dates, that’s YYYY-MM-DD). This way, when you sort by name, the names will also be sorted chronologically. Only use dates in non-ISO 8601 formats in code when needing to write in ‘regular’ English, like in body text or plot labels (and in this case, write out the month names so that it’s unambiguous internationally).\n\n\nSecurity\nDo not expose any private information in your code. This includes any PII (personally identifiable information) and any API keys or passwords you may use for 3rd-party services.\nIf you need to refer to PII (e.g., to reference a specific address), use an anonymous key instead (e.g., a participant ID).\nAPI keys should be stored in your .REnviron file and be called from there. Packages like tidycensus will automatically call your key from the .REnviron file. API keys are private information and you don’t want others to potentially misuse your API key. The easiest way to edit your .REnviron file to run usethis::edit_r_environ()."
  },
  {
    "objectID": "code-guidelines.html#version-control",
    "href": "code-guidelines.html#version-control",
    "title": "Guidelines for code",
    "section": "Version control",
    "text": "Version control\nIf the project you are working on has a Github repository, use it—all your code for that project should live in that repo. Speak with your colleagues on the project beforehand to make sure how you plan to organize your files within the repository makes sense, and to ascertain the best way for you to manage your contributions (e.g., using branches named in a certain way).\nIf your project does not yet have a repository, it probably should. Speak with your colleagues about the structure that makes the most sense for your project.\nWhen you’re working on a task, it is best practice to use a branch that corresponds to that task. When you’re finished with the task, use the pull request system to coordinate the review of your code and the incorporation of your changes into main.\nYou should use the Github Desktop app to manage your interface with Git, rather than the built-in RStudio interface (which is missing several important features).\nFor more on Git and Github, see the Using Github page on this website."
  },
  {
    "objectID": "code-guidelines.html#ordering-and-organizing",
    "href": "code-guidelines.html#ordering-and-organizing",
    "title": "Guidelines for code",
    "section": "Ordering and organizing",
    "text": "Ordering and organizing\nHow exactly your scripts should be organized will vary greatly by project. Check in with your colleagues from time to time about the organization of project files. Usually this is not considered a particularly exciting topic, but good organization that adapts to changing circumstances is well worth putting in the occasional time. Remember that projects often do not go away as scheduled! Someone (probably you) may need to revive the project 6 (or 10, or 24) months later. The less you have to reconstruct the project then, the better.\nOne generally applicable principle, however, is to keep major steps of the data science pipeline (cleaning, EDA, analysis, modeling, communication) separate. In a smaller project, this might be one script or RMD per step; in a larger project, each step may have multiple scripts organized in directories.\n\n\n\n\n\n\nTip\n\n\n\nIf there are multiple scripts per directory, prefix them numerically (starting with ‘00_’ or ‘01_’) so that they display in order. The order should be the order the scripts should be executed in.\n\n\nRemember that the data that you input/output from your scripts into Box should also be well organized. Keep a separate readme in the project data folder describing each of the datasources as well as the intermediate outputs, as well as a link to the project repo. The readme in the project repo should also have a link to the Box data folder."
  },
  {
    "objectID": "code-guidelines.html#scripts-vs-reports",
    "href": "code-guidelines.html#scripts-vs-reports",
    "title": "Guidelines for code",
    "section": "Scripts vs reports",
    "text": "Scripts vs reports\nGenerally, if what you want to do is communicate some aspect of our data (EDA, or a polished analysis) to a larger audience (the rest of your team or to the public), you want to write in a format that supports interleaving code and expository writing: for R, this will be Quarto or R Markdown.\nHowever, for material that is not explicitly communicative in nature, I recommend using regular .R scripts instead. This is because RMD/Quarto files are paradoxically less legible and more cumbersome if you’re interested only in the code (for example, RMDs will not have syntax highlighting when viewed on Github)."
  },
  {
    "objectID": "code-guidelines.html#know-your-data",
    "href": "code-guidelines.html#know-your-data",
    "title": "Guidelines for code",
    "section": "Know your data!",
    "text": "Know your data!\nThe very first step is to know what you’re working with: that should be your first goal when you start a project. Make sure you understand the following:\n\nWhat’s the data for?\n\nThe goals of the project\nWhat’s the purpose of the research? What are the research questions? What facts will you need to answer these questions? Make sure you are on the same page as as your colleagues.\n\n\nDomain-specific information\nWhat program, policy, or entity is this data about? Get background knowledge. If the data come from a program, what does the program do? Who’s eligible? Who funds it? When did it start? Ask your colleagues to share with you any background material.\n\n\nThe data source\nWho gave you the data? Who collected it? How did you get it? Were the data compiled by someone before you got it? If so, who did it and how was it done? Often, the datasource will be a project partner, or a colleague. Again, ask your colleagues; see if any questions about the data can be addressed by asking the datasource.\n\n\nMetadata\nThis can be a data dictionary, a description of the data collection procedures, survey questionnaires, or even notes from meetings with the datasource. But be careful! Just because there is a data dictionary, it doesn’t mean that it’s accurate. Data dictionaries are seductive because they promise authoritative information, but they are fallible. The definitions of fields often change as data are collected. There can also be outright errors in data dictionaries, even from venerable sources like the Census Bureau. Verify before you trust.\n\n\n\nWhat’s in the data?\nExpanding upon this theme: it is unwise to assume anything about the data that you haven’t checked out for yourself. This is especially the case if you think you do have reasons to trust the datasource. Whenever you work with a dataset, you should convince yourself that you understand the following:\n\nWhat you have\nDo you have the number of rows you expect to have? Is the file suspiciously too large or too small? Is it older or newer than you expected? Is the filename weird? Are there any warnings when you import the data?\nIf your dataset is less than ~1000 rows and ~50 columns, it’s a good idea to manually skim through the entire dataset when you first import it. If your dataset is larger, look first at a few hundred contiguous records, and then pull out a random chunk (e.g., with dplyr::slice_sample) to manually examine.\nIf your data are updated with newer data, you should be ready to encounter new anomalies; have procedures in hand to check incoming data before you rely on them.\n\n\nWhat represents a record\nWhat do you expect each row to represent, and is this actually the case? If there is a unique ID, is it actually unique? If there isn’t one, can you reliably create one from the data, or do you need more information? Are there duplicates among the rows which should belong to a unique record? If there are, why? And what do you need to know to be able to understand the duplicates and resolve them? (A helpful tool for this is janitor::get_dupes().)\n\n\nWhat are the record attributes\nIn short, what are the columns? Does the formatting of the column names make sense? (If they do, you can use janitor::clean_names() to standardize them to snake_case. If they don’t make sense, you should investigate further.) How many columns are there and what are the types? (You can use skimr::skim() to answer this question.) If the data types are not what you expect (e.g., ZIP codes stored as numerics or dates stored in Excel format), are they fixable? Are there any columns which appear duplicated, or could refer to the same thing? If so, verify that they are actually exactly the same, or find out exactly how they differ.\n\n\n\n\n\n\nWarning\n\n\n\nBe careful about assuming data types based on what they look like. A GEOID column, for example, may only have numeric digits but should be stored as strings. Check the types explicitly.\n\n\n\n\nMissing data\nHow does your dataset encode missing data? In some datasets, missing data are specific values, like ‘-b’ or ‘-1’. Sometimes missing values can vary by variable, like the lowest possible value for that column minus one. In others, it may be keywords like NULL, Null, NaN, nan, N/A, n/a, N/a, or any variation thereof. Often, a dataset will have several different ways to encode NAs.\nOne of the first things you should do is to standardize all missing values (but only the missing values) to R’s NA keyword. Once that’s done, are there any rows or columns that are completely NA? (janitor::remove_empty(), with verbosity on, is helpful.) If so, is it safe to remove them? Inspect the skim output and notice any variables with large NA percentages. But just as suspicious are variables with zero NAs: were missing values for these variables coded badly (e.g., did the datasource assume that NA means FALSE?)? In some cases, it might be ambiguous whether a value like 0 is really a zero or actually represents missing or inapplicable data. In such a case you may need to address a question to the datasource.\n\n\nUnexpected values\nFor numeric variables, sort by ascending and descending and notice any outliers. A simple histogram can also be a good tool. Are people reporting negative number of children? Is there anyone earning $10 million in income? Are 30% of the rows reporting $0 in rent? Are there implausible dates? For categorical variables, how many unique values are there for each variable? How common is each unique variable value? The skim output, and janitor::tabyl(), can get you quick initial answers for many of these questions. You may also find some more issues with nonstandard NA values at this point.\n\n\nGeographic data\nFor any datasets with geospatial information, make sure you know which coordinate reference system (CRS) is applicable—don’t assume, for example, that a GEOJSON file has coordinates in WGS84 (even though it should). Before you work with geospatial data, map the data and check they’re located where you expect them to be (e.g., Philadelphia, not Null Island).\n\n\nRelationship with other data\nIf records in the current dataset need to be joined to records in another, is there a common join key? If there is, how many fail to join from each dataset and is there an explanation why? If there is not a pre-existing join key, do you have the information you need to construct one? If a field in your current dataset also exists in another dataset, do they agree with each other, for every matched record?\n\n\n\nThe data cycle\nThere’s a common saying that 90% of data work is data cleaning and 10% is analysis. That’s false. It’s closer to the truth to say that 50% is understanding what the data contain in the first place, 45% is cleaning, and 5% is analysis, though the exact proportion will differ between projects.\nThere is an order of operationws: You should start getting to know the data before you clean it, and you should be comfortable with data quality before you do any analysis. But the work will be iterative—you may find some unexpected pattern while you clean, and structuring an analysis may suggest ways to understand the data differently. Don’t be afraid of revisiting prior steps with better knowledge, but at the same time, understand that decisions you make at the earliest stages are also the most difficult to change later. Keep your colleagues updated throughout so that everyone on the project is on the same page."
  },
  {
    "objectID": "code-guidelines.html#na-handling",
    "href": "code-guidelines.html#na-handling",
    "title": "Guidelines for code",
    "section": "NA handling",
    "text": "NA handling\nOn to a new topic. How missing data are treated in data processing steps is an important but easily-neglected issue: mistakes in this arena can introduce consequential errors, but are very easy to miss unless you’re looking for them.\nOnce you understand the missingness patterns in your data, make sure that your code handles them properly. How R handles missing data can be unintuitive at times, so whenever you construct any logical comparison, or generate any summary, make sure you you’re satisfied how your code dealt with NAs.\nRemember, NAs are not values—they are labels for we don't know. That’s why NA == NA is a meaningless expression and we use is.na() instead.\nThe following are common examples of how NA errors can creep in when you don’t watch out for them:\n\nBe very careful when filtering based on columns that contain NAs: rows will always drop if the logical comparison evaluates to NA.\n\nFor example, if you want to drop anyone younger than 18, filter(data, age &gt;= 18) will also drop any records with NA for age. Almost never do you actually want this, so you’ll need to write instead filter(data, age &gt;= 18 | is.na(age)).\nIf you do want to drop rows with NAs, explicitly note this in a comment, and include in the comment how many rows are dropped.\n\nMost R operations will propagate NAs (e.g. the mean() of any vector containing NA will itself be NA), but not always.\n\nThe %in% operator will never return NA. Evaluating NA %in% NA will return TRUE! This may be what you want, but keep in mind that %in% is not interchangeable with element-wise |, which can return NA.\nDplyr’s join family will unfortunately match NAs in the join key(s) with each other, so that everyone with a missing join key in dataframe A will match with everyone missing a join key in dataframe B. This is the opposite of SQL behavior. If there’s a chance that your join key(s) will include any NAs, set the argument na_matches = \"never\".\n\nGenerally, don’t forget that NAs can be introduced even if you don’t specify such conditions explicitly. Don’t expect ifelse() or case_when() to always output non-missing values.\n\nTo help mitigate any NA-driven errors, keep an eye on the tidylog output to make sure that your filter or mutate steps did what you expected. Whenever you introduce, drop, or transform NAs, explicitly note in a comment how many rows are affected."
  },
  {
    "objectID": "code-guidelines.html#dirty-environments",
    "href": "code-guidelines.html#dirty-environments",
    "title": "Guidelines for code",
    "section": "Dirty environments",
    "text": "Dirty environments\nLet’s now look more closely at reproducibility. We write code so that our work can be performed again and again with the same results, across users and machines. Sometimes, this requires some pre-planning to achieve. Below are some requirements to keep in mind, divided by considerations that apply to code within a file, versus those that apply to the interrelationships between files.\n\nWithin-file\n\nFilepaths\nNearly every script you write will require the specification of some filepaths, in order to read in (or write out) data. Of course, the exact content of the filepath will differ between computers and between operating systems. This means that if you ever call a ‘hard’ (or ‘absolute’) filepath in your script (e.g., /Users/yourname/Documents/Projects/Repos/code-guidelines), it will by definition not work for anyone not using your computer.\nThere are two methods that can deal with this. If your project and all of the needed data can live in one directory (or be called by API), the easiest solution is to use an R project. When you create an R project, and open associated scripts in the R project window in RStudio, the R project will bring along its own location so that only relative paths need to be called, which will be the same for everyone using the project directory. This is a useful short guide to using R Projects. However, this situation will rarely be the case at HIP.\nUsually, your data will need to be read from Box, not from within your project directory. For our work at HIP, any data containing sensitive information must be stored on Box; therefore, most projects tracked under Git will need to interface with Box. An API could be used to do this, but the simpler solution is to use Box Drive to enable users to call locations on Box using a regular filepath, and to store the locally-specific part of the Box Drive filepath as a local environment variable in the .REnviron file. The Getting Set Up page on this website tells you how to set this up.\n\n\n\n\n\n\nWarning\n\n\n\nWhen storing or writing any path, use the unix directory separator / instead of the Windows separator \\. The former can be read cross-platform in R but not the latter.\n\n\n\n\nWriting in place\nIt is unfortunately possible create an object, and later alter the object, under the same name. For example:\n\ndata &lt;- data.frame(a = c(1, 2, 3, 4))\n\ndata$b &lt;- c(10, 11, NA, NA)\n\ndata &lt;- dplyr::filter(data, !is.na(b))\n\nIn this example, the same object named data is created or modified at three different places. The example above may look relatively harmless, but is in fact very risky. Consider that dozens—or hundreds—of lines of code may intervene between the three statements. Would you be confident of knowing at any given point whether data has 1 or 2 columns, or whether it has 4 rows or 2?\nConsider that writing code almost always develops interactively, and you may go back and forth between different sections of your script, continuously accumulating unreproducible changes to data. You will lose track of what data contains under such circumstances. This will then lead to code that doesn’t work—or worse: works but without you realizing it does something completely different than what you intended.\nModifying an existing object without assigning it a new name is called ‘writing in place’. This is dangerous. Simple rule: never write in place.\n\n\n\n\n\n\nImportant\n\n\n\nNever make changes to an object without assigning it a different name.\n\n\nThe names of your objects should always have a 1-to-1 relationship with their states. If the state of your objects is not clear, not only will it cause confusion for you and your reader, it is likely that errors will be introduced which are difficult to fix—and worse, difficult to spot.\nInstead, always assign new names to objects when you change them. The example above could be fixed like this:\n\ndata_raw &lt;- data.frame(a = c(1, 2, 3, 4))\n\ndata_prepared &lt;- data_raw %&gt;% \n  mutate(b = c(10, 11, NA, NA)) %&gt;% \n  # filter: removed 2 rows (50%), 2 rows remaining\n  filter(!is.na(b))\n\nThis way, an object will have only one set of contents, always.\nIn R, pipes let you do this very naturally and simply. Sometimes the rule of avoiding changing in place means that pipelines may need to be broken out if you need some intermediate computation. That’s fine: having to create one more name is small price to pay to achieve transparent code. When a name is only used once (remember, a pipeline is a single expression), you and your reader will know exactly what the name refers to at all times. It does mean that you may spend some more time thinking of good object names, but that is itself helpful in guiding the reader through what your code does.\n\n\n\n\n\n\nCaution\n\n\n\nIn other programming languages like Python, be aware that associating an existing object with a new name actually maintains the link to the old object, and changing the object under the new name also changes the object under the old name. This is one disadvantage of that language compared to R.\n\n\n\n\nRNG seeds\nSome functions will use randomness to do their jobs. Of course, true randomness would be incompatible with reproducibility, but any function that uses a random number generator (really, a pseudorandom number generator) can be constrained by setting the RNG seed beforehand, either by explicitly using set.seed() just before the function is called, or sometimes by setting a seed as a function argument. This will constrain the ‘random’ result of the function so that the result is the same across runs.\n\n\n\n\n\n\nWarning\n\n\n\nRemember that a seed value set through set.seed() is ‘consumed’ when it is used, so it will need to be reset every time when an RNG is used, not just once at the beginning of the script.\n\n\n\n\n\nBetween-files\nAny but the smallest projects will have more than one script associated with it. It’s crucial for replicability that the relationship between the files is transparent and well-defined. Here are some principles to keep in mind:\n\nScripts should be executed one at a time, and the order of execution should be reflected by the name of the script. As stated above, use numeric prefixes to indicate this. For more complex projects it’s a good idea to have separate directories to separate scripts belonging to different stages in the data analytics workflow.\nScripts should always have explicit inputs and outputs. A script should always start running from a fresh R session, with no objects loaded. Any output from a previous step should be written out in the earlier script as an intermediate and be read in from the later script.\nThe state of your RStudio Environment panel should never be treated as authoritative. What’s authoritative is your code, and any files that are written out from your code. What’s real is what’s on the hard drive, not what’s in RAM.\n\n\n\n\n\n\n\nTip\n\n\n\nThe shortcut cmd-shift-0 (ctrl-shift-F10) will restart R and clear your environment, right where you are without restarting RStudio. You should do this every once in a while while you’re developing scripts to guard against unintended environment-related errors, which can be very difficult to ferret out. Always restart your R session before running a script to write out data.\n\n\nAn important corollary is that the origin of any data files and intermediate files should be clear. A readme file specifying where each data/intermediate file comes from will accomplish this. It’s also a good idea to organize intermediates with appropriate directory structures, corresponding to how your source scripts are organized.\n\n\n\n\n\n\nWarning\n\n\n\nIf an intermediate file contains PII (for example, cleaned addresses), it is sensitive data and can only be stored in Box.\n\n\n\n\nPackages and functions\nA full treatment of package environments in R is beyond the scope of this guide, but a few things to keep in mind are:\n\nAvoid using functions or features that the package maintainer has labeled deprecated.\nBe careful of functions or packages that are under very active development, as future changes may break backwards compatibility with your code.\nTools like renv enhance reproducibility by letting you specify exactly which versions and sources your pakages come from."
  },
  {
    "objectID": "code-guidelines.html#grouped-data",
    "href": "code-guidelines.html#grouped-data",
    "title": "Guidelines for code",
    "section": "Grouped data",
    "text": "Grouped data\nOne last topic: grouped data. Remember that once a dataframe is grouped, the grouping is persistent, even if the object is copied to a different name. It is best practice to always ungroup() any dataframe immediately after you are done with operations that use the grouping. No dataframe should leave a pipeline in a grouped state. Very difficult-to-unravel bugs can be caused when you forget that a grouped dataframe is grouped and you apply a function that acts on a grouping that no longer makes sense.\n\n\n\n\n\n\nTip\n\n\n\nIn many situations, using the by = or .by = argument to specify grouping within a particular function is more concise than calling group_by() then ungroup() one step later in the pipeline."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Working with data at HIP",
    "section": "",
    "text": "Working with data at HIP\n\nWelcome! This website collects information and resources to help you do great quantitative work at HIP.\nThere are four pages on this site:\n\nHow to get set up with your tools\nHow to use Github\nGuidelines for writing code\nA library of external resources\n\nThe website was created and is maintained by Chi-Hyun Kim. For any questions or suggestions, please don’t hesitate to contact him."
  }
]